import os
import pdfplumber
import pandas as pd
import json
import re
import time
from typing import List, Dict, Any
from openai import OpenAI

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

LEVEL_ENUM = ["global standard", "national", "local", "industry", "company specific"]

# -----------------------------
# 1) æå– PDF æ®µè½ + é¡µç 
# -----------------------------
def extract_paragraphs_from_pdf(pdf_path: str) -> List[Dict[str, Any]]:
    paras = []
    with pdfplumber.open(pdf_path) as pdf:
        for i, page in enumerate(pdf.pages):
            text = page.extract_text()
            if not text:
                continue
            for para in text.split("\n"):
                cleaned = (para or "").strip()
                if len(cleaned) < 5:
                    continue
                if re.fullmatch(r"[\W_]+", cleaned):
                    continue
                paras.append({"page": i + 1, "text": cleaned})
    return paras

def clean_json_output(text: str) -> str:
    text = (text or "").strip()
    if not text:
        return "[]"
    if text.startswith("```"):
        text = re.sub(r"^```[a-zA-Z]*\n?", "", text)
        text = re.sub(r"\n?```$", "", text)
    return text.strip()

# -----------------------------
# 2) Yes/No åˆ†ç±»ï¼ˆå¸¦ä¸Šä¸‹æ–‡ï¼‰
# -----------------------------
def build_yesno_prompt(paragraph: str, context_prev: str, context_next: str) -> str:
    return f"""
You are an ESG analyst. Determine whether the CURRENT paragraph likely contains quantitative or structural ESG indicator data
(e.g., emissions Scope 1/2/3, energy use, water, waste, recycling, training hours, injury rates, board composition, targets, KPIs).

Answer ONLY "Yes" or "No".

Previous context:
\"\"\"{context_prev}\"\"\"

Current paragraph:
\"\"\"{paragraph}\"\"\"

Next context:
\"\"\"{context_next}\"\"\"
""".strip()

def is_potential_esg_paragraph(
    paragraph: str,
    context_prev: str = "",
    context_next: str = "",
    model: str = "gpt-4o",
) -> bool:
    prompt = build_yesno_prompt(paragraph, context_prev, context_next)
    resp = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
    )
    ans = (resp.choices[0].message.content or "").strip().lower()
    return ans.startswith("yes")

# -----------------------------
# 3) Yes çš„æ®µè½æ‰æŠ½å– rowsï¼ˆå« definition/level/pageï¼‰
# -----------------------------
def build_extraction_prompt(paragraph: str, context_prev: str, context_next: str, page: int) -> str:
    return f"""
You are an ESG analyst. You will be given a paragraph from a corporate sustainability report, plus surrounding context.

Task:
Extract ALL quantitative or structural ESG indicators mentioned (ONLY those with clearly stated values or counts/percentages/targets).
For EACH indicator, output one JSON object with the following fields:
- "indicator_name": short human-readable name
- "definition": 1â€“2 sentences based ONLY on paragraph + context. If not defined, keep minimal and do NOT invent details.
- "values": exact value string as stated (preserve units)
- "page": integer page number (use the provided page)
- "level": choose EXACTLY ONE from:
  ["global standard", "national", "local", "industry", "company specific"]

How to assign "level":
- "global standard": explicitly tied to global frameworks/standards (GHG Protocol, GRI, SASB, ISSB, TCFD, CDP, UN SDGs, ISO, etc.)
- "national": national laws/regulations/country-level mandatory reporting
- "local": state/province/city/municipal/local authority requirements
- "industry": sector/industry standards or supply-chain frameworks (e.g., RBA)
- "company specific": internal KPIs/targets/figures without external standard/regulation anchors

Output format:
Return ONLY a valid JSON array. If nothing extractable, return [].

Page: {page}

Previous context:
\"\"\"{context_prev}\"\"\"

Current paragraph:
\"\"\"{paragraph}\"\"\"

Next context:
\"\"\"{context_next}\"\"\"
""".strip()

def extract_esg_rows_from_paragraph(
    paragraph: str,
    context_prev: str,
    context_next: str,
    page: int,
    model: str = "gpt-4o",
    max_retries: int = 2,
    sleep_sec: float = 1.0,
) -> List[Dict[str, Any]]:
    prompt = build_extraction_prompt(paragraph, context_prev, context_next, page)

    last_err = None
    for _ in range(max_retries + 1):
        try:
            resp = client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0,
            )
            content = clean_json_output(resp.choices[0].message.content)
            parsed = json.loads(content)

            if not isinstance(parsed, list):
                return []

            normalized = []
            for r in parsed:
                if not isinstance(r, dict):
                    continue
                indicator_name = str(r.get("indicator_name", "")).strip()
                definition = str(r.get("definition", "")).strip()
                values = str(r.get("values", "")).strip()
                level = str(r.get("level", "")).strip()

                if not indicator_name or not values:
                    continue
                if level not in LEVEL_ENUM:
                    level = "company specific"

                normalized.append({
                    "indicator_name": indicator_name,
                    "definition": definition,
                    "values": values,
                    "page": int(page),
                    "level": level,
                })

            return normalized

        except Exception as e:
            last_err = e
            time.sleep(sleep_sec)

    raise RuntimeError(f"LLM extraction failed after retries: {last_err}")

# -----------------------------
# 4) ä¿å­˜
# -----------------------------
def save_intermediate_results(rows: List[Dict[str, Any]], output_path: str) -> None:
    if not rows:
        return
    df = pd.DataFrame(rows, columns=["indicator_name", "definition", "values", "page", "level"])
    df.to_excel(output_path, index=False)

# -----------------------------
# 5) ä¸»æµç¨‹ï¼šå…ˆ Yes/Noï¼Œå†æŠ½å–
# -----------------------------
def main():
    script_dir = os.path.dirname(os.path.abspath(__file__))
    pdf_path = os.path.join(script_dir, "luxshare_sustainability_report_2024.pdf")
    output_path = os.path.join(script_dir, "esg_extracted_results_yesno_post.xlsx")

    if not os.path.exists(pdf_path):
        print(f"âŒ æ‰¾ä¸åˆ° PDF æ–‡ä»¶: {pdf_path}")
        return

    paras = extract_paragraphs_from_pdf(pdf_path)
    print(f"ðŸ“„ æ®µè½æ•°: {len(paras)}")

    extracted_rows: List[Dict[str, Any]] = []

    processed = 0
    yes_count = 0
    no_count = 0
    error_count = 0

    # ä¸Šä¸‹æ–‡çª—å£ï¼šå‰åŽå„ N æ®µï¼ˆä½ å¯ä»¥æ”¹æˆ 2ï¼‰
    context_window = 1

    # ä¿å­˜é¢‘çŽ‡ï¼šæ¯ç´¯è®¡å¤šå°‘æ¡â€œæŒ‡æ ‡ rowâ€ä¿å­˜ä¸€æ¬¡
    save_interval_rows = 30

    for i, item in enumerate(paras):
        processed += 1
        page = item["page"]
        text = item["text"]

        # ç»„ä¸Šä¸‹æ–‡
        prev_texts = []
        next_texts = []
        for k in range(1, context_window + 1):
            if i - k >= 0:
                prev_texts.append(paras[i - k]["text"])
            if i + k < len(paras):
                next_texts.append(paras[i + k]["text"])

        context_prev = "\n".join(reversed(prev_texts))
        context_next = "\n".join(next_texts)

        try:
            # å…ˆ Yes/No
            if not is_potential_esg_paragraph(
                paragraph=text,
                context_prev=context_prev,
                context_next=context_next,
                model="gpt-4o",   # æƒ³çœé’±å¯æ¢æˆæ›´ä¾¿å®œçš„å°æ¨¡åž‹ï¼ˆå¦‚æžœä½ è´¦å·å¯ç”¨ï¼‰
            ):
                no_count += 1
                if no_count % 200 == 0:
                    print(f"â© å·²åˆ¤ No: {no_count}")
                continue

            yes_count += 1
            print(f"âœ… Yes | ç¬¬ {page} é¡µ | å¼€å§‹æŠ½å–...")

            # Yes æ‰æŠ½å– rows
            rows = extract_esg_rows_from_paragraph(
                paragraph=text,
                context_prev=context_prev,
                context_next=context_next,
                page=page,
                model="gpt-4o",
            )

            if rows:
                before = len(extracted_rows)
                extracted_rows.extend(rows)
                added = len(extracted_rows) - before
                print(f"   âœ“ æŠ½åˆ° {added} æ¡æŒ‡æ ‡ï¼ˆç´¯è®¡ {len(extracted_rows)}ï¼‰")
                for r in rows[:2]:
                    print(f"   â€¢ {r['indicator_name']} | {r['values']} | {r['level']}")
            else:
                print(f"   âš ï¸ Yes ä½†æ²¡æŠ½åˆ°æŒ‡æ ‡ï¼ˆç¬¬ {page} é¡µï¼‰")

            # å¢žé‡ä¿å­˜
            if len(extracted_rows) > 0 and (len(extracted_rows) % save_interval_rows) < max(len(rows), 1):
                save_intermediate_results(extracted_rows, output_path)
                print(f"ðŸ’¾ å·²è‡ªåŠ¨ä¿å­˜: {output_path}")

            if processed % 50 == 0:
                print(f"â³ processed {processed}/{len(paras)} | Yes {yes_count} | No {no_count} | rows {len(extracted_rows)} | errors {error_count}")

        except Exception as e:
            error_count += 1
            print(f"âŒ å‡ºé”™ | ç¬¬ {page} é¡µ: {e}")

    # æœ€ç»ˆä¿å­˜
    if extracted_rows:
        save_intermediate_results(extracted_rows, output_path)
        print("\nðŸŽ‰ å®Œæˆï¼")
        print(f"   Yes: {yes_count}, No: {no_count}, Errors: {error_count}")
        print(f"   æŒ‡æ ‡è¡Œæ•°: {len(extracted_rows)}")
        print(f"   è¾“å‡º: {output_path}")
    else:
        df = pd.DataFrame(columns=["indicator_name", "definition", "values", "page", "level"])
        df.to_excel(output_path, index=False)
        print("\nâš ï¸ æ²¡æŠ½åˆ°ä»»ä½•æŒ‡æ ‡ï¼Œå·²ä¿å­˜ç©ºè¡¨å¤´:", output_path)

if __name__ == "__main__":
    main()
